# Mechanistic Interpretability: Four-Phase Proof

## Executive Summary
Systematic analysis across geometric, feature, circuit, and causal dimensions demonstrates that neural networks are not black boxes, but mechanistic systems that can be understood and controlled.

## Key Results
- **Phase 1:** Data manifolds untangle (PCA, UMAP, separability)
- **Phase 2:** Neurons encode interpretable features (heatmaps, selectivity)
- **Phase 3:** Modular circuits implement tasks (circuit diagrams, ablation)
- **Phase 4:** Knowledge is editable (causal tracing, surgical fix)

## Outputs
- All figures: `visualizations/`
- All summaries: `analysis/`

## How to Reproduce
1. Install dependencies (see README.md)
2. Run each phase script in order
3. Review outputs in `visualizations/` and `analysis/`

## Takeaway
Neural networks are transparent, interpretable, and controllable.
